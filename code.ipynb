{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "728d1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks, optimizers, backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Add, Concatenate\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "import keras_tuner as kt\n",
    "DATA_DIR = 'data'\n",
    "MODEL_DIR = 'models'\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "class modelResults:\n",
    "    def __init__(self,model,pred):\n",
    "        self.model = model\n",
    "        self.pred = pred\n",
    "\n",
    "\n",
    "\n",
    "def read_file(csvFileName):\n",
    "    return pd.read_csv(DATA_DIR+'/'+csvFileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING \n",
    "\n",
    "def cleanColumns(data):\n",
    "       # Drop 'Unnamed: 0' column\n",
    "    #remove brackets\n",
    "    if \"Unnamed: 0\" in data.columns:\n",
    "        data = data.drop([\"Unnamed: 0\"], axis=\"columns\")\n",
    "    columns = data.columns\n",
    "    data.columns =  [re.sub(r'\\s*\\([^)]*\\)', '', col).strip() for col in columns]\n",
    "    return data\n",
    "\n",
    "def applyConstraints(data):\n",
    "    return data[(data['Tank Width'] > 0) & \n",
    "                (data['Tank Length'] > 0) & \n",
    "                (data['Tank Height'] > 0) & \n",
    "                (data['Vapour Height'] >= 0) &\n",
    "                (data['Vapour Temperature'] > 0) &\n",
    "                (data['Liquid Temperature'] > 0)]\n",
    "\n",
    "def standardizeStatus(data):\n",
    "    data[\"Status\"] = data[\"Status\"].str.lower().str.replace(' ', '')\n",
    "    data['Status'] = data['Status'].apply(lambda x: 'superheated' if 'h' in x else ('subcooled' if 'c' in x else x))\n",
    "    return data[\"Status\"]\n",
    "\n",
    "def remove_outliers_iqr(df, numeric_cols):\n",
    "    cleaned_df = df.copy()\n",
    "    total_outliers = 0\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        Q1 = cleaned_df[col].quantile(0.25)\n",
    "        Q3 = cleaned_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 3 * IQR\n",
    "\n",
    "        outliers = cleaned_df[(cleaned_df[col] < lower_bound) | (cleaned_df[col] > upper_bound)]\n",
    "        cleaned_df = cleaned_df[(cleaned_df[col] >= lower_bound) & (cleaned_df[col] <= upper_bound)]\n",
    "\n",
    "        outlier_count = len(outliers)\n",
    "        if outlier_count > 0:\n",
    "            print(f\"\\nColumn: {col}\")\n",
    "            print(f\"  IQR range: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "            print(f\"  Count of outliers: {outlier_count}\")\n",
    "            print(f\"  Sample outlier values:\\n{outliers[col].head(5).to_string(index=False)}\")\n",
    "\n",
    "        total_outliers += outlier_count\n",
    "\n",
    "    print(f\"\\nTotal outliers removed: {total_outliers}\")\n",
    "    return cleaned_df\n",
    "\n",
    "def clean_data(data):\n",
    "    # Initial row count\n",
    "    initial_rows = len(data)\n",
    "    print(f\"Initial rows: {initial_rows}\")\n",
    "\n",
    "    # Cleanup columns\n",
    "    data = cleanColumns(data)\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    before_na = len(data)\n",
    "    data = data.dropna(axis=0)\n",
    "    after_na = len(data)\n",
    "    print(f\"Rows removed due to NA: {before_na - after_na}\")\n",
    "\n",
    "    # Logical constraints\n",
    "    before_constraints = len(data)\n",
    "    data = applyConstraints(data)\n",
    "    after_constraints = len(data)\n",
    "    print(f\"Rows removed due to logical constraints: {before_constraints - after_constraints}\")\n",
    "\n",
    "    #Duplicates\n",
    "    before_duplicates = len(data)\n",
    "    data = data.drop_duplicates()\n",
    "    after_duplicates = len(data)\n",
    "    print(f\"Rows removed as duplicates: {before_duplicates - after_duplicates}\")\n",
    "\n",
    "    # Standardize 'Status' column\n",
    "    data[\"Status\"] = standardizeStatus(data)\n",
    "\n",
    "\n",
    "    numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    before_outliers = len(data)\n",
    "    data = remove_outliers_iqr(data, numeric_cols)\n",
    "    after_outliers = len(data)\n",
    "    print(f\"Rows removed as outliers: {before_outliers - after_outliers}\")\n",
    "\n",
    "    # Final row count\n",
    "    final_rows = len(data)\n",
    "    print(f\"Final rows: {final_rows}\")\n",
    "    print(f\"Total rows removed after cleaning up: {initial_rows - final_rows}\")\n",
    "    return data\n",
    "\n",
    "# def remove_highly_correlated_features(df, threshold=0.9):\n",
    "#     corr_matrix = df.corr(numeric_only=True).abs()\n",
    "#     upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "#     to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "#     print(\"Highly correlated features to drop:\", to_drop)\n",
    "#     return df.drop(columns=to_drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b5b510a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(data):\n",
    "    \n",
    "    data[\"Liquid Boiling Temperature\"] = data[\"Liquid Boiling Temperature\"] +273.15\n",
    "    data[\"Liquid Critical Temperature\"] = data[\"Liquid Critical Temperature\"] +273.15\n",
    "    data[\"Tank Volume\"] = data[\"Tank Width\"] * data[\"Tank Length\"] * data[\"Tank Height\"]  \n",
    "    data[\"HeightRatio\"]= data[\"Vapour Height\"] / data[\"Tank Height\"] \n",
    "    data[\"Superheat Margin\"] = data[\"Liquid Temperature\"] - data[\"Liquid Boiling Temperature\"]\n",
    "    # Total Energy (approximate thermal energy in the tank)\n",
    "    data[\"Liquid Volume\"] = data[\"Liquid Ratio\"] * data[\"Tank Volume\"]\n",
    "    data[\"Total Energy\"] = data[\"Liquid Volume\"] * data[\"Superheat Margin\"]\n",
    "\n",
    "    # (BLEVE assumed at tank center top)\n",
    "    data[\"Sensor Distance to BLEVE\"] = (\n",
    "        data[\"Sensor Position x\"]**2 +\n",
    "        data[\"Sensor Position y\"]**2 +\n",
    "        (data[\"Sensor Position z\"] - data[\"BLEVE Height\"])**2\n",
    "    ) ** 0.5\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess(data,train=True,feature_eng = True):\n",
    "    if train:\n",
    "        data = clean_data(data)\n",
    "    data = pd.get_dummies(data, columns=[\"Sensor Position Side\"], prefix=\"Side\")\n",
    "    data = pd.get_dummies(data, columns=[\"Status\"], prefix=\"Status\")\n",
    "    print(data.head(1))\n",
    "    if feature_eng:\n",
    "        data = feature_engineer(data)\n",
    "    y =  None\n",
    "    if (train):\n",
    "        y= data[ \"Target Pressure\"] \n",
    "        data = data.drop([\"Target Pressure\"], axis=\"columns\")\n",
    "    # StandardScaler for inputs\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    #split data\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(\"Training Set Size:\", X_train.shape)\n",
    "    print(\"Validation Set Size:\", X_val.shape)\n",
    "    return  X_train, X_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a7da4bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows: 10050\n",
      "Rows removed due to NA: 110\n",
      "Rows removed due to logical constraints: 0\n",
      "Rows removed as duplicates: 50\n",
      "\n",
      "Column: Tank Failure Pressure\n",
      "  IQR range: [-16.27, 72.11]\n",
      "  Count of outliers: 48\n",
      "  Sample outlier values:\n",
      "1446.628788\n",
      "3404.750305\n",
      "1722.348566\n",
      "4095.753928\n",
      "2379.656661\n",
      "\n",
      "Column: Sensor Position y\n",
      "  IQR range: [-9.47, 18.65]\n",
      "  Count of outliers: 10\n",
      "  Sample outlier values:\n",
      "19.3\n",
      "19.3\n",
      "19.3\n",
      "19.3\n",
      "19.3\n",
      "\n",
      "Column: Target Pressure\n",
      "  IQR range: [-0.36, 1.04]\n",
      "  Count of outliers: 661\n",
      "  Sample outlier values:\n",
      "1.093819\n",
      "1.091484\n",
      "1.036741\n",
      "1.204910\n",
      "1.179709\n",
      "\n",
      "Total outliers removed: 719\n",
      "Rows removed as outliers: 719\n",
      "Final rows: 9171\n",
      "Total rows removed after cleaning up: 879\n",
      "   Tank Failure Pressure  Liquid Ratio  Tank Width  Tank Length  Tank Height  \\\n",
      "0                  14.26          0.25        1.58         8.61         1.79   \n",
      "\n",
      "   BLEVE Height  Vapour Height  Vapour Temperature  Liquid Temperature  \\\n",
      "0          1.01            1.4              457.14              423.07   \n",
      "\n",
      "   Obstacle Distance to BLEVE  Obstacle Width  Obstacle Height  \\\n",
      "0                        19.0            11.0              5.0   \n",
      "\n",
      "   Obstacle Thickness  Obstacle Angle  Liquid Critical Pressure  \\\n",
      "0                0.39             1.0                      37.9   \n",
      "\n",
      "   Liquid Boiling Temperature  Liquid Critical Temperature  Sensor ID  \\\n",
      "0                        -1.0                        152.0       18.0   \n",
      "\n",
      "   Sensor Position x  Sensor Position y  Sensor Position z  Target Pressure  \\\n",
      "0              19.75                5.7                3.1         0.588298   \n",
      "\n",
      "   Side_1.0  Side_2.0  Side_3.0  Side_4.0  Side_5.0  Status_subcooled  \\\n",
      "0     False      True     False     False     False             False   \n",
      "\n",
      "   Status_superheated  \n",
      "0                True  \n",
      "Training Set Size: (7336, 34)\n",
      "Validation Set Size: (1835, 34)\n"
     ]
    }
   ],
   "source": [
    "### LOAD DATA HERE\n",
    "trainData = read_file('train.csv')\n",
    "trainData = preprocess(trainData)\n",
    "X_train, X_val, y_train, y_val= trainData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dc4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM and XgbBoost\n",
    "\n",
    "\n",
    "def summarize_results(name, best_estimator, mse, r2):\n",
    "    if hasattr(best_estimator, 'get_params'):\n",
    "        params = best_estimator.get_params()\n",
    "    else:\n",
    "        params = {}\n",
    "\n",
    "    summary = {\n",
    "        'Model': name,\n",
    "        'MSE': mse,\n",
    "        'R2': r2,\n",
    "        'Best Params': str(params)\n",
    "    }\n",
    "    return pd.DataFrame([summary])\n",
    "\n",
    "def train_xgboost():\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "    grid_search = GridSearchCV(model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def train_svr():\n",
    "    param_grid = {\n",
    "        'svr__C': [1, 10, 100],\n",
    "        'svr__epsilon': [0.01, 0.1,0.5],\n",
    "        'svr__kernel': ['rbf','poly']\n",
    "    }\n",
    "\n",
    "    pipeline = make_pipeline(SVR())\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    return best_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7afafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "\n",
    "def build_nn():\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(256, activation='mish', input_shape=(X_train.shape[1],),\n",
    "                    kernel_regularizer=regularizers.l2(1e-5)),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(256, activation='mish', kernel_regularizer=regularizers.l2(1e-5)),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(256, activation='mish', kernel_regularizer=regularizers.l2(1e-5)),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(1, activation='softplus')\n",
    "    ])\n",
    "\n",
    "    optimizer = optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    # optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "    # model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Callbacks\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_nn(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training & Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training & Validation MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Training & Validation MAE')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_nn(model):\n",
    "    early_stop = callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=100, restore_best_weights=True\n",
    "    )\n",
    "    history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1)\n",
    "    plot_nn(history)\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = load_model('best.keras')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the validation dataset only\n",
    "val_loss, val_mae = model.evaluate(X_val, y_val, batch_size=512, verbose=1)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation MAE: {val_mae}\")\n",
    "\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=2000,\n",
    "#     batch_size=512,\n",
    "#     callbacks=[early_stop],\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36878c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('best.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdbe5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd6b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have X_test and y_test prepared\n",
    "# 1. Evaluate the model on the test set\n",
    "test_data =   pd.read_csv(\"test.csv\")\n",
    "y_test = pd.read_csv(\"sample_prediction.csv\")\n",
    "predData = y_test[\"Target Pressure (bar)\"]\n",
    "test_scaled,_ = preprocess(test_data,True)\n",
    "test_loss, test_mae = model.evaluate(test_scaled, predData)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "\n",
    "\n",
    "y_pred = model.predict(test_scaled).flatten()  # Ensure it's a 1D array\n",
    "\n",
    "# 2. Get the true target values\n",
    "y_true = y_test[\"Target Pressure (bar)\"].values\n",
    "\n",
    "\n",
    "\n",
    "# 3. Calculate MAPE (excluding zero targets to avoid division by zero)\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "# 4. Compute and print MAPE\n",
    "mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "print(f\"MAPE: {mape/10}\")\n",
    "\n",
    "\n",
    "# Assuming the ID column in the test data is named 'ID'\n",
    "# 1. Create a DataFrame with the predictions and IDs\n",
    "output_df = pd.DataFrame({\n",
    "    'ID': y_test['ID'],  # Assuming 'ID' is in the test_data\n",
    "    'Target Pressure (bar)': y_pred  # The predicted values\n",
    "})\n",
    "\n",
    "# 2. Save the DataFrame to a CSV file\n",
    "output_df.to_csv('predictions_output.csv', index=False)\n",
    "print(\"Predictions have been saved to 'predictions_output.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aadd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Tunable units\n",
    "    for i in range(hp.Int(\"num_layers\", 2, 4)):\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=128, max_value=512, step=64),\n",
    "            activation='mish',\n",
    "            kernel_regularizer=regularizers.l2(hp.Choice('l2_reg', [1e-5, 1e-4, 1e-3]))\n",
    "        ))\n",
    "        model.add(layers.Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='softplus'))\n",
    "    \n",
    "    # Compile\n",
    "    optimizer = optimizers.SGD(\n",
    "        learning_rate=hp.Choice('learning_rate', [0.01, 0.05, 0.1]),\n",
    "        momentum=hp.Float('momentum', min_value=0.5, max_value=0.95, step=0.05)\n",
    "    )\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='mish_model'\n",
    ")\n",
    "\n",
    "# Register Mish\n",
    "tf.keras.utils.get_custom_objects().update({'mish': mish})\n",
    "\n",
    "# Run search\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[callbacks.EarlyStopping(monitor='val_loss', patience=10)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "for key in best_hps.values:\n",
    "    print(f\"{key}: {best_hps.get(key)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
