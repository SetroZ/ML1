{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, callbacks, optimizers, backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Add, Concatenate\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "\n",
    "import keras_tuner as kt\n",
    "DATA_DIR = 'data'\n",
    "MODEL_DIR = 'models'\n",
    "PRED_DIR = 'predictions'\n",
    "# Display settings\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "\n",
    "\n",
    "def get_unique_path(base_path):\n",
    "    if not os.path.exists(base_path):\n",
    "        return base_path\n",
    "    name, ext = os.path.splitext(base_path)\n",
    "    version = 2\n",
    "    new_path = f\"{name}_V{version}{ext}\"\n",
    "    while os.path.exists(new_path):\n",
    "        version += 1\n",
    "        new_path = f\"{name}_V{version}{ext}\"\n",
    "    return new_path\n",
    "\n",
    "def saveModel(model, fileName):\n",
    "    base_path = os.path.join(MODEL_DIR, fileName)\n",
    "    path = get_unique_path(base_path)\n",
    "\n",
    "    if 'nn' in fileName:\n",
    "        model.save(path)\n",
    "    else:\n",
    "        joblib.dump(model, path)\n",
    "        \n",
    "\n",
    "def loadModel(modelName):\n",
    "    path = os.path.join(MODEL_DIR, modelName)\n",
    "    if 'nn' in modelName:\n",
    "        return load_model(path)\n",
    "    else:\n",
    "        return joblib.load(path)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def read_file(csvFileName):\n",
    "    return pd.read_csv(DATA_DIR+'/'+csvFileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING \n",
    "\n",
    "def cleanColumns(data):\n",
    "       # Drop 'Unnamed: 0' column\n",
    "    #remove brackets\n",
    "    if \"Unnamed: 0\" in data.columns:\n",
    "        data = data.drop([\"Unnamed: 0\"], axis=\"columns\")\n",
    "    columns = data.columns\n",
    "    data.columns =  [re.sub(r'\\s*\\([^)]*\\)', '', col).strip() for col in columns]\n",
    "    return data\n",
    "\n",
    "def applyConstraints(data):\n",
    "    return data[(data['Tank Width'] > 0) & \n",
    "                (data['Tank Length'] > 0) & \n",
    "                (data['Tank Height'] > 0) & \n",
    "                (data['Vapour Height'] >= 0) &\n",
    "                (data['Vapour Temperature'] > 0) &\n",
    "                (data['Liquid Temperature'] > 0)]\n",
    "\n",
    "def standardizeStatus(data):\n",
    "    data[\"Status\"] = data[\"Status\"].str.lower().str.replace(' ', '')\n",
    "    data['Status'] = data['Status'].apply(lambda x: 'superheated' if 'h' in x else ('subcooled' if 'c' in x else x))\n",
    "    return data[\"Status\"]\n",
    "\n",
    "def remove_outliers_iqr(df, numeric_cols):\n",
    "    cleaned_df = df.copy()\n",
    "    total_outliers = 0\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        Q1 = cleaned_df[col].quantile(0.25)\n",
    "        Q3 = cleaned_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers = cleaned_df[(cleaned_df[col] < lower_bound) | (cleaned_df[col] > upper_bound)]\n",
    "        cleaned_df = cleaned_df[(cleaned_df[col] >= lower_bound) & (cleaned_df[col] <= upper_bound)]\n",
    "\n",
    "        outlier_count = len(outliers)\n",
    "        if outlier_count > 0:\n",
    "            print(f\"\\nColumn: {col}\")\n",
    "            print(f\"  IQR range: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "            print(f\"  Count of outliers: {outlier_count}\")\n",
    "            print(f\"  Sample outlier values:\\n{outliers[col].head(5).to_string(index=False)}\")\n",
    "\n",
    "        total_outliers += outlier_count\n",
    "\n",
    "    print(f\"\\nTotal outliers removed: {total_outliers}\")\n",
    "    return cleaned_df\n",
    "\n",
    "def clean_data(data):\n",
    "    # Initial row count\n",
    "    initial_rows = len(data)\n",
    "    print(f\"Initial rows: {initial_rows}\")\n",
    "\n",
    "    # Cleanup columns\n",
    "    data = cleanColumns(data)\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    before_na = len(data)\n",
    "    data = data.dropna(axis=0)\n",
    "    after_na = len(data)\n",
    "    print(f\"Rows removed due to NA: {before_na - after_na}\")\n",
    "\n",
    "    # Logical constraints\n",
    "    before_constraints = len(data)\n",
    "    data = applyConstraints(data)\n",
    "    after_constraints = len(data)\n",
    "    print(f\"Rows removed due to logical constraints: {before_constraints - after_constraints}\")\n",
    "\n",
    "    #Duplicates\n",
    "    before_duplicates = len(data)\n",
    "    data = data.drop_duplicates()\n",
    "    after_duplicates = len(data)\n",
    "    print(f\"Rows removed as duplicates: {before_duplicates - after_duplicates}\")\n",
    "\n",
    "    # Standardize 'Status' column\n",
    "    data[\"Status\"] = standardizeStatus(data)\n",
    "\n",
    "\n",
    "    numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "    before_outliers = len(data)\n",
    "    data = remove_outliers_iqr(data, numeric_cols)\n",
    "    after_outliers = len(data)\n",
    "    print(f\"Rows removed as outliers: {before_outliers - after_outliers}\")\n",
    "\n",
    "    # Final row count\n",
    "    final_rows = len(data)\n",
    "    print(f\"Final rows: {final_rows}\")\n",
    "    print(f\"Total rows removed after cleaning up: {initial_rows - final_rows}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b510a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(data):\n",
    "    \n",
    "    data[\"Liquid Boiling Temperature\"] = data[\"Liquid Boiling Temperature\"] +273.15\n",
    "    data[\"Liquid Critical Temperature\"] = data[\"Liquid Critical Temperature\"] +273.15\n",
    "    data[\"Tank Volume\"] = data[\"Tank Width\"] * data[\"Tank Length\"] * data[\"Tank Height\"]  \n",
    "    data[\"HeightRatio\"]= data[\"Vapour Height\"] / data[\"Tank Height\"] \n",
    "    data[\"Superheat Margin\"] = data[\"Liquid Temperature\"] - data[\"Liquid Boiling Temperature\"]\n",
    "    # Total Energy (approximate thermal energy in the tank)\n",
    "    data[\"Liquid Volume\"] = data[\"Liquid Ratio\"] * data[\"Tank Volume\"]\n",
    "    data[\"Total Energy\"] = data[\"Liquid Volume\"] * data[\"Superheat Margin\"]\n",
    "\n",
    "    # (BLEVE assumed at tank center top)\n",
    "    data[\"Sensor Distance to BLEVE\"] = (\n",
    "        data[\"Sensor Position x\"]**2 +\n",
    "        data[\"Sensor Position y\"]**2 +\n",
    "        (data[\"Sensor Position z\"] - data[\"BLEVE Height\"])**2\n",
    "    ) ** 0.5\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess(data,train=True,feature_eng = True):\n",
    "\n",
    "    if train:\n",
    "        data = clean_data(data)\n",
    "    data = pd.get_dummies(data, columns=[\"Sensor Position Side\"], prefix=\"Side\")\n",
    "    data = pd.get_dummies(data, columns=[\"Status\"], prefix=\"Status\")\n",
    "    print(data.head(1))\n",
    "    if feature_eng:\n",
    "        data = feature_engineer(data)\n",
    "    y =  None\n",
    "    if (train):\n",
    "        y= data[ \"Target Pressure\"] \n",
    "        data = data.drop([\"Target Pressure\"], axis=\"columns\")\n",
    "    \n",
    "    # StandardScaler for inputs\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    #split data\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(\"Training Set Size:\", X_train.shape)\n",
    "    print(\"Validation Set Size:\", X_val.shape)\n",
    "    return  X_train, X_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da4bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD DATA HERE\n",
    "trainData = read_file('train.csv')\n",
    "trainData = preprocess(trainData,train=True,feature_eng=True)\n",
    "X_train, X_val, y_train, y_val= trainData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a1dc4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_grid_search_table(grid_search,modelName, metric='mean_test_score', top_n=20):\n",
    "    # Extract and sort results\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    results = results.sort_values(by=metric, ascending=False).head(top_n)\n",
    "\n",
    "    # Get only param columns and the score\n",
    "    table_data = results.filter(like='param_').copy()\n",
    "    table_data[metric] = results[metric].values\n",
    "\n",
    "    # Reset index for clean display\n",
    "    table_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Convert all values to string for uniform display\n",
    "    table_data = table_data.astype(str)\n",
    "\n",
    "    # Plot table as figure\n",
    "    fig, ax = plt.subplots(figsize=(min(25, table_data.shape[1]*2), top_n * 0.5 + 1))\n",
    "    ax.axis('off')\n",
    "\n",
    "    table = ax.table(cellText=table_data.values,\n",
    "                     colLabels=table_data.columns,\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.5)\n",
    "\n",
    "    plt.title(f\"Top {top_n} GridSearchCV {modelName} Results by {metric}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def summarize_results(model,name):\n",
    "    y_pred = model.predict(X_val)\n",
    "    # Compute metrics\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RÂ²: {r2:.4f}\")\n",
    "    summary = {\n",
    "        'Model': name,\n",
    "        'MSE': mse,\n",
    "        'R2': r2,\n",
    "        'Best Params': str(model.getparams())\n",
    "    }\n",
    "    return pd.DataFrame([summary])\n",
    "\n",
    "def train_xgboost():\n",
    "    # param_grid = {\n",
    "    #     'n_estimators': [100, 300, 500],\n",
    "    #     'max_depth': [3, 6, 9],\n",
    "    #     'learning_rate': [0.01, 0.1, 0.3],\n",
    "    #     'subsample': [0.8],\n",
    "    #     'colsample_bytree': [0.8],\n",
    "    #     'min_child_weight': [1],\n",
    "    #     'reg_lambda': [1],\n",
    "    #     'reg_alpha': [0]\n",
    "    # }\n",
    "    ## Fine TUnning\n",
    "    param_grid = {\n",
    "    'n_estimators': [500,700],\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'colsample_bylevel': [0.7, 0.8, 0.9],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'reg_lambda': [0.1, 1, 10],\n",
    "    'reg_alpha': [0, 0.1, 1]\n",
    "}\n",
    "\n",
    "    model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "    grid_search = GridSearchCV(model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    return best_model , grid_search\n",
    "\n",
    "\n",
    "def train_svr():\n",
    "    param_grid = {\n",
    "        'svr__C': [0.01,0.1,1, 10, 100],\n",
    "        'svr__epsilon': [0.01, 0.1,0.5],\n",
    "        'svr__kernel': ['rbf','poly']\n",
    "    }\n",
    "\n",
    "    pipeline = make_pipeline(SVR())\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    return best_model , grid_search\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 39366 candidates, totalling 118098 fits\n"
     ]
    }
   ],
   "source": [
    "xgb_model,grid_search = train_xgboost()\n",
    "plot_grid_search_table(grid_search,\"XGBBoost\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d40ae356",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(xgb_model,'xgb_best.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7afafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "\n",
    "def build_nn():\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(256, activation='mish', input_shape=(X_train.shape[1],),\n",
    "                    kernel_regularizer=regularizers.l2(1e-5)),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(256, activation='mish', kernel_regularizer=regularizers.l2(1e-5)),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(256, activation='mish', kernel_regularizer=regularizers.l2(1e-5)),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(1, activation='softplus')\n",
    "    ])\n",
    "\n",
    "    optimizer = optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    # optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "    # model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Callbacks\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_nn(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training & Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Training & Validation MAE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['mae'], label='Training MAE')\n",
    "    plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "    plt.title('Training & Validation MAE')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_nn(model):\n",
    "    early_stop = callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=100, restore_best_weights=True\n",
    "    )\n",
    "    history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=500,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2)\n",
    "    plot_nn(history)\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048f0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING\n",
    "svr_model = train_svr()\n",
    "saveModel(svr_model,'svr.pkl')\n",
    "summarize_results(svr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948cd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbc02d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da2e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_nn(nn_model)\n",
    "\n",
    "val_loss, val_mae = nn_model.evaluate(X_val, y_val, batch_size=512, verbose=1)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation MAE: {val_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming models array contains [xgb_model, svr_model, nn_model]\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "val_losses = []\n",
    "val_maes = []\n",
    "val_mapes = []\n",
    "model_names = [\"XGBoost\", \"SVR\", \"Neural Network\"]\n",
    "models=[xgb_model,svr_model,nn_model]\n",
    "\n",
    "# Loop through models and calculate metrics\n",
    "for model in models:\n",
    "    y_pred = model.predict(X_val).flatten()  # Ensure predictions are 1D\n",
    "    val_loss = mean_squared_error(y_val, y_pred)\n",
    "    val_mae = mean_absolute_error(y_val, y_pred)\n",
    "    val_mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_maes.append(val_mae)\n",
    "    val_mapes.append(val_mape)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Histogram for val_loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(model_names, val_losses, color='skyblue')\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Model\")\n",
    "\n",
    "# Histogram for val_mae\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(model_names, val_maes, color='orange')\n",
    "plt.title(\"Validation MAE\")\n",
    "plt.ylabel(\"Mean Absolute Error\")\n",
    "plt.xlabel(\"Model\")\n",
    "\n",
    "# Histogram for val_mape\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(model_names, val_mapes, color='green')\n",
    "plt.title(\"Validation MAPE\")\n",
    "plt.ylabel(\"Mean Absolute Percentage Error (%)\")\n",
    "plt.xlabel(\"Model\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd6b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model(model,modelName):\n",
    "    test_data = read_file('test.csv')\n",
    "    y_test = pd.read_csv(\"sample_prediction.csv\")\n",
    "    test_data = preprocess(test_data,False,False)\n",
    "    y_pred = model.predict(test_scaled).flatten()\n",
    "    test_scaled,_ = preprocess(data=test_data,train=False,feature_eng=True)\n",
    "    output_df = pd.DataFrame({\n",
    "    'ID': y_test['ID'], \n",
    "    'Target Pressure (bar)': y_pred \n",
    "})\n",
    "    fileName = get_unique_path(os.path.join(PRED_DIR,f'{modelName}_pred.csv'))\n",
    "    output_df.to_csv(fileName, index=False)\n",
    "    print(f'Predictions have been saved to ${fileName}.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aadd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Tunable units\n",
    "    for i in range(hp.Int(\"num_layers\", 2, 4)):\n",
    "        model.add(layers.Dense(\n",
    "            units=hp.Int(f'units_{i}', min_value=128, max_value=512, step=64),\n",
    "            activation='mish',\n",
    "            kernel_regularizer=regularizers.l2(hp.Choice('l2_reg', [1e-5, 1e-4, 1e-3]))\n",
    "        ))\n",
    "        model.add(layers.Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='softplus'))\n",
    "    \n",
    "    # Compile\n",
    "    optimizer = optimizers.SGD(\n",
    "        learning_rate=hp.Choice('learning_rate', [0.01, 0.05, 0.1]),\n",
    "        momentum=hp.Float('momentum', min_value=0.5, max_value=0.95, step=0.05)\n",
    "    )\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='mish_model'\n",
    ")\n",
    "\n",
    "# Register Mish\n",
    "tf.keras.utils.get_custom_objects().update({'mish': mish})\n",
    "\n",
    "# Run search\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[callbacks.EarlyStopping(monitor='val_loss', patience=10)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "for key in best_hps.values:\n",
    "    print(f\"{key}: {best_hps.get(key)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf277d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
